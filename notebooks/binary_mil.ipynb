{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Convolution1D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import pescador\n",
    "from scipy.stats import zscore\n",
    "import gzip\n",
    "import os\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mil_bag_generator(cqtfile, actfile, n_bag_frames, min_active_frames, n_hop_frames, \n",
    "                      zscore_std=False, shuffle=True):\n",
    "    '''\n",
    "    Generate a MIL bag with corresponding label. \n",
    "    The function yields a dictionary with three elements: \n",
    "    X = features, Y = label, Z = bag ID (trackid + first frame index).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cqtfile : str\n",
    "        Path to .npy.gz file containing the log-CQT matrix\n",
    "    actfile : str\n",
    "        Path to .npy.gz file containing the activation vector\n",
    "    n_bag_frames : int\n",
    "        Number of frames to include in a bag\n",
    "    min_active_frames: int\n",
    "        Minimum number of consecutive active frames to consider bag positive\n",
    "    n_hop_frames : int\n",
    "        Number of frames to jump between consecutive bags\n",
    "    zscore : bool\n",
    "        Whether or not to standardize the bag features (zscore)\n",
    "    shuffle : bool\n",
    "        Whether to shuffle the ordering of the bags (for sgd) or not (for\n",
    "        validation and test)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bag : dictionary with X = features, Y = label, Z = bag ID\n",
    "    '''\n",
    "    # Load cqt file and ativation file\n",
    "    cqt = np.load(gzip.open(cqtfile, 'rb'))\n",
    "    act = np.load(gzip.open(actfile, 'rb'))\n",
    "\n",
    "    order = np.arange(0, cqt.shape[1]-n_bag_frames, n_hop_frames) # librosa puts time in dim 1\n",
    "    # Randomize frame order\n",
    "    if shuffle:\n",
    "        np.random.shuffle(order)\n",
    "    # Get bag ID (from filename)\n",
    "    trackid = '_'.join(os.path.basename(cqtfile).split('_')[:2])\n",
    "\n",
    "    for frame in order:\n",
    "        \n",
    "        # Carve out TF patch, standardize (optional) and reshape\n",
    "        patch = cqt[:, frame:frame + n_bag_frames]\n",
    "        if zscore_std:\n",
    "            patch = zscore(patch, axis=None)\n",
    "        patch = patch.reshape(-1, patch.shape[0], patch.shape[1], 1)\n",
    "        \n",
    "        # Compute bag label\n",
    "        patch_act = act[frame:frame+n_bag_frames]\n",
    "        condition = patch_act >= 0.5\n",
    "        # The following computes the length of every consecutive sequence\n",
    "        # of True's in condition:\n",
    "        active_lengths = np.diff(np.where(np.concatenate(\n",
    "            ([condition[0]], condition[:-1] != condition[1:], [True])))[0])[::2]\n",
    "        # Need at least min_active_frames to consider bag as positive\n",
    "        if len(active_lengths) > 0:\n",
    "            bag_label = 1 * (active_lengths.max() >= min_active_frames)\n",
    "        else:\n",
    "            bag_label = 0\n",
    "        \n",
    "        # Compute bag ID\n",
    "        bagid = '{:s}_{:d}'.format(trackid, frame)    \n",
    "        \n",
    "        yield dict(\n",
    "            X=patch,\n",
    "            Y=np.asarray([bag_label], dtype=np.int32),\n",
    "            ID=np.asarray([bagid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_mux(streams, batch_size, n_samples=None, n_active=1000,\n",
    "              with_replacement=False):\n",
    "    '''\n",
    "    Multiplex streams into batches of size n_batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    streams : list of pescador.Streamer\n",
    "        The list of streams to multiplex\n",
    "    batch_size : int > 0\n",
    "        Number of samples to in each batch (batch size)\n",
    "    n_samples : int or None\n",
    "        Number of individual samples to generate (limit). If None, generate\n",
    "        infinite number of samples (unless with_replacement is False in which\n",
    "        case generate until all streams are exhausted)\n",
    "    n_active : int > 0\n",
    "        Number of streams that can be active simultaneously\n",
    "    with_replacement : bool\n",
    "        If true sample form streams indefinitely. If False streams are sampled\n",
    "        until exhausted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_streamer : pescador.Streamer\n",
    "        Batch multiplexer\n",
    "    '''\n",
    "\n",
    "    stream_mux = pescador.Streamer(\n",
    "        pescador.mux, streams, n_samples, n_active,\n",
    "        with_replacement=with_replacement)\n",
    "\n",
    "    batch_streamer = pescador.Streamer(\n",
    "        pescador.buffer_streamer, stream_mux, batch_size)\n",
    "\n",
    "    return batch_streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vad_minibatch_generator(root_folder, track_list,\n",
    "                            augmentations=['original'],\n",
    "                            feature='cqt44100_1024_8_36', \n",
    "                            activation='vocal_activation44100_1024',\n",
    "                            n_bag_frames=44,\n",
    "                            min_active_frames=2,\n",
    "                            n_hop_frames=22,\n",
    "                            zscore_std=False,\n",
    "                            shuffle=True,\n",
    "                            batch_size=100, \n",
    "                            n_samples=None,\n",
    "                            n_active=1000,\n",
    "                            with_replacement=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder\n",
    "    track_list\n",
    "    augmentations\n",
    "    feature\n",
    "    activation\n",
    "    n_bag_frames\n",
    "    min_activate_frames\n",
    "    n_hop_frames\n",
    "    zscore_std\n",
    "    shuffle\n",
    "    batch_size\n",
    "    n_samples\n",
    "    n_active\n",
    "    with_replacement\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    # DEBUG\n",
    "    print(\"Collecting feature files...\")\n",
    "    \n",
    "    # Collect all feature files\n",
    "    cqt_files = []\n",
    "    for aug in augmentations:\n",
    "        aug_folder = os.path.join(root_folder, aug, 'features', feature)\n",
    "        for root, dirnames, filenames in os.walk(aug_folder):\n",
    "            for filename in fnmatch.filter(filenames, '*cqt.npy.gz'):\n",
    "                cqt_files.append(os.path.join(root, filename))\n",
    "                \n",
    "    # DEBUG\n",
    "    print(\"Found {:d} files\".format(len(cqt_files)))\n",
    "    print(\"Creating streams...\")\n",
    "\n",
    "    # Turn all files into streams\n",
    "    streams = []\n",
    "    for cqtfile in cqt_files:\n",
    "        # get matching activation file\n",
    "        actfolder = os.path.join(os.path.dirname(os.path.dirname(cqtfile)), activation)\n",
    "        actfile = os.path.join(actfolder, os.path.basename(cqtfile).replace('_cqt.npy.gz', \n",
    "                                                                            '_vocalactivation.npy.gz'))\n",
    "        assert os.path.isfile(actfile)\n",
    "        streams.append(pescador.Streamer(mil_bag_generator, cqtfile, actfile,\n",
    "                                         n_bag_frames, min_active_frames, n_hop_frames, \n",
    "                                          zscore_std, shuffle))\n",
    "        \n",
    "    # DEBUG\n",
    "    print(\"Done\")\n",
    "\n",
    "    # Mux the streams into minimbatches\n",
    "    batch_streamer = batch_mux(streams, batch_size, n_samples=n_samples,\n",
    "                               n_active=n_active, \n",
    "                               with_replacement=with_replacement)\n",
    "\n",
    "    return batch_streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_vad_minibatch_generator(root_folder, track_list,\n",
    "                                  augmentations=['original'],\n",
    "                                  feature='cqt44100_1024_8_36', \n",
    "                                  activation='vocal_activation44100_1024',\n",
    "                                  n_bag_frames=44,\n",
    "                                  min_active_frames=2,\n",
    "                                  n_hop_frames=22,\n",
    "                                  zscore_std=False,\n",
    "                                  shuffle=True,\n",
    "                                  batch_size=100, \n",
    "                                  n_samples=None,\n",
    "                                  n_active=1000,\n",
    "                                  with_replacement=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_folder\n",
    "    track_list\n",
    "    augmentations\n",
    "    feature\n",
    "    activation\n",
    "    n_bag_frames\n",
    "    min_activate_frames\n",
    "    n_hop_frames\n",
    "    zscore_std\n",
    "    shuffle\n",
    "    batch_size\n",
    "    n_samples\n",
    "    n_active\n",
    "    with_replacement\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    keras_generator = vad_minibatch_generator(\n",
    "        root_folder, track_list, augmentations, feature, activation,\n",
    "        n_bag_frames, min_active_frames, n_hop_frames, zscore_std,\n",
    "        shuffle, batch_size, n_samples, n_active, with_replacement)\n",
    "    \n",
    "    for batch in keras_generator.generate():\n",
    "        yield (batch['X'], batch['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = np.load('../data/dataSplits_7_1_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN GENERATOR\n",
    "root_folder = os.path.expanduser('~/datasets/MedleyDB_output/')\n",
    "track_list = split[2][0]\n",
    "augmentations = ['original']\n",
    "feature = 'cqt44100_1024_8_36'\n",
    "activation = 'vocal_activation44100_1024'\n",
    "n_bag_frames = 44\n",
    "min_active_frames = 2\n",
    "n_hop_frames = 22\n",
    "zscore_std= False\n",
    "shuffle = True\n",
    "batch_size = 100\n",
    "n_samples = None\n",
    "n_active = 1000\n",
    "with_replacement = False\n",
    "\n",
    "train_generator = keras_vad_minibatch_generator(\n",
    "    root_folder, track_list, augmentations, feature, activation,\n",
    "    n_bag_frames, min_active_frames, n_hop_frames, zscore_std,\n",
    "    shuffle, batch_size, n_samples, n_active, with_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VALIDATE GENERATOR\n",
    "root_folder = os.path.expanduser('~/datasets/MedleyDB_output/')\n",
    "track_list = split[2][1]\n",
    "augmentations = ['original']\n",
    "feature = 'cqt44100_1024_8_36'\n",
    "activation = 'vocal_activation44100_1024'\n",
    "n_bag_frames = 44\n",
    "min_active_frames = 2\n",
    "n_hop_frames = 22\n",
    "zscore_std= False\n",
    "shuffle = True\n",
    "batch_size = 100\n",
    "n_samples = None\n",
    "n_active = 1000\n",
    "with_replacement = False\n",
    "\n",
    "validate_generator = keras_vad_minibatch_generator(\n",
    "    root_folder, track_list, augmentations, feature, activation,\n",
    "    n_bag_frames, min_active_frames, n_hop_frames, zscore_std,\n",
    "    shuffle, batch_size, n_samples, n_active, with_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feature files...\n",
      "Found 37 files\n",
      "Creating streams...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: get one batch\n",
    "for n, batch in enumerate(train_generator):\n",
    "    X_train = batch[0]\n",
    "    Y_train = batch[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 288, 44, 1) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define softmax pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _keras_smp(x):\n",
    "    m = K.max(x, axis=-1, keepdims=True)\n",
    "    sm = K.exp(x - m)\n",
    "    w = sm / K.sum(sm, axis=-1, keepdims=True)\n",
    "    return K.sum(x * w, axis=-1, keepdims=True)\n",
    "    \n",
    "def _keras_smp_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    shape[-1] = 1\n",
    "    return tuple(shape)\n",
    "#     return tuple(shape[:-1])\n",
    "#     return (input_shape[0], 1)\n",
    "\n",
    "SoftMaxPool = Lambda(_keras_smp, output_shape=_keras_smp_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _keras_squeeze(x, axis=1):\n",
    "    return K.squeeze(x, axis=axis)\n",
    "\n",
    "def _keras_squeeze_shape(input_shape):\n",
    "    shape = np.array(input_shape)\n",
    "    shape = shape[shape!=1]\n",
    "    return tuple(shape)\n",
    "\n",
    "SqueezeLayer = Lambda(_keras_squeeze, output_shape=_keras_squeeze_shape, arguments={'axis': 1})\n",
    "SqueezeLastLayer = Lambda(_keras_squeeze, output_shape=_keras_squeeze_shape, arguments={'axis': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input TF dimensions\n",
    "tf_rows, tf_cols = 288, 44\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "# pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "fullheight_kernel_size = (tf_rows, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 44, 1)\n"
     ]
    }
   ],
   "source": [
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape = (1, tf_rows, tf_cols)\n",
    "else:\n",
    "    input_shape = (tf_rows, tf_cols, 1)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=input_shape)\n",
    "y1 = Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode='same', activation='relu', name='y1')(inputs)\n",
    "y2 = Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode='same', activation='relu', name='y2')(y1)\n",
    "y3 = Convolution2D(nb_filters, fullheight_kernel_size[0], fullheight_kernel_size[1], \n",
    "                  border_mode='valid', activation='relu', name='y3')(y2)\n",
    "y4 = SqueezeLayer(y3)\n",
    "y5 = Convolution1D(1, 1, border_mode='valid', activation='sigmoid', name='y5')(y4)\n",
    "y6 = SqueezeLastLayer(y5)\n",
    "y7 = SoftMaxPool(y6)\n",
    "predictions = Activation('sigmoid', name='predictions')(y7)\n",
    "\n",
    "model = Model(input=inputs, output=predictions)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_23 (InputLayer)            (None, 288, 44, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "y1 (Convolution2D)               (None, 288, 44, 32)   320         input_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "y2 (Convolution2D)               (None, 288, 44, 32)   9248        y1[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "y3 (Convolution2D)               (None, 1, 44, 32)     294944      y2[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)               (None, 44, 32)        0           y3[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "y5 (Convolution1D)               (None, 44, 1)         33          lambda_25[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)               (None, 44)            0           y5[0][0]                         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)               (None, 1)             0           lambda_26[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Activation)         (None, 1)             0           lambda_24[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 304,545\n",
      "Trainable params: 304,545\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s - loss: 0.6932 - acc: 0.6400     \n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 9s - loss: 0.6931 - acc: 0.6100     \n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 10s - loss: 0.6931 - acc: 0.6100    \n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 9s - loss: 0.6931 - acc: 0.6100     \n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 9s - loss: 0.6931 - acc: 0.6100     \n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 9s - loss: 0.6931 - acc: 0.6100     \n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 9s - loss: 0.6931 - acc: 0.6100     \n",
      "Epoch 8/10\n",
      " 60/100 [=================>............] - ETA: 3s - loss: 0.6931 - acc: 0.6000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-cb30ad1c105e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/justin/dev/miniconda3/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin/dev/miniconda3/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin/dev/miniconda3/envs/py35/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin/dev/miniconda3/envs/py35/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=10, nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
